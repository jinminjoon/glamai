{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeonseosla/opt/anaconda3/envs/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time \n",
    "import html\n",
    "from datetime import datetime\n",
    "\n",
    "import pymysql\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if getattr(sys, 'frozen', False) and hasattr(sys, '_MEIPASS'):\n",
    "    root = sys._MEIPASS\n",
    "else:\n",
    "    root = os.path.dirname(os.path.realpath('__file__'))\n",
    "    src  = os.path.join(root, 'src')\n",
    "    sys.path.append(src)\n",
    "\n",
    "from database.access import AccessDatabase\n",
    "from crawling.crawler import get_url, json_iterator\n",
    "db_glamai = AccessDatabase('glamai')\n",
    "db_jangho = AccessDatabase('jangho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Sephora sale update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateProductSale:\n",
    "    def __init__(self):\n",
    "        self.__conn__()\n",
    "        \n",
    "    def __conn__(self):\n",
    "        self.ds = AccessDatabase('glamai')\n",
    "        self.ds_conn, self.ds_cur = self.ds._connect()\n",
    "        \n",
    "    def __close__(self):\n",
    "        self.ds_conn.commit()\n",
    "        self.ds_cur.close()\n",
    "        self.ds_conn.close()\n",
    "\n",
    "    def insert_data_new(self, vertical):\n",
    "        # insert new product\n",
    "        query = f'''\n",
    "                    insert into sephora_{vertical}_data_sale (product_code, item_no, list_price, regist_date)\n",
    "                    select product_code, item_no, price, regist_date from sephora_{vertical}_data_status\n",
    "                    where is_use=1 and (product_code, item_no) not in (select product_code, item_no from sephora_{vertical}_data_sale);\n",
    "                '''\n",
    "        while True:\n",
    "            try:\n",
    "                self.ds_cur.execute(query)\n",
    "                self.ds_conn.commit()\n",
    "                print(f'{vertical} new product update 완료!')\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(100)\n",
    "                self.__close__()\n",
    "                self.__conn__()\n",
    "                print(\"DB 연결 끊김 ... 재연결 성공!\")\n",
    "        \n",
    "    def get_data(self, vertical):\n",
    "        table = f'sephora_{vertical}_data_status'\n",
    "        query = f'select distinct(product_code) from {table} where is_use=1;'\n",
    "        \n",
    "        while True:\n",
    "            try:   \n",
    "                self.ds_cur.execute(query)\n",
    "                product_codes = list(self.ds_cur.fetchall())\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(100)\n",
    "                self.__close__()\n",
    "                self.__conn__()\n",
    "                print(\"DB 연결 끊김 ... 재연결 성공!\")\n",
    "        \n",
    "        return product_codes\n",
    "\n",
    "    def scraper_price(self, data, product_code):\n",
    "        try:\n",
    "            current_sku = str(data['currentSku'])\n",
    "            status = 1\n",
    "        except KeyError:\n",
    "            status = 0\n",
    "\n",
    "        if status == 1:\n",
    "            listPrice_pattern = r\"\\'listPrice\\': \\'\\$[0-9]{0,5}.[0-9]{0,5}\"\n",
    "            listPrice_re = re.search(listPrice_pattern, current_sku)\n",
    "            if listPrice_re is None:\n",
    "                listPrice = float(0)\n",
    "            else:\n",
    "                listPrice = listPrice_re.group()\n",
    "                listPrice = float(listPrice.split(\":\")[1].replace(\"'\", \"\").replace(\" \", \"\").replace(\"$\",\"\"))\n",
    "                \n",
    "            salePrice_pattern = r\"\\'salePrice\\': \\'\\$[0-9]{0,5}.[0-9]{0,5}\"\n",
    "            salePrice_re = re.search(salePrice_pattern, current_sku)\n",
    "            if salePrice_re is None:\n",
    "                salePrice = float(0)\n",
    "                is_sale = 0\n",
    "            else:\n",
    "                salePrice = salePrice_re.group()\n",
    "                salePrice = float(salePrice.split(\":\")[1].replace(\"'\", \"\").replace(\" \", \"\").replace(\"$\",\"\"))\n",
    "                is_sale = 1\n",
    "                \n",
    "            skuId_pattern = r\"\\'skuId\\': \\'[0-9]{5,10}\"\n",
    "            skuId_re = re.search(skuId_pattern, current_sku)\n",
    "            if skuId_re is None:\n",
    "                skuId = None\n",
    "            else:\n",
    "                skuId = skuId_re.group()\n",
    "                item_no = int(skuId.split(\":\")[1].replace(\"'\", \"\").replace(\" \", \"\"))\n",
    "                \n",
    "            return [listPrice, salePrice, is_sale, datetime.now(), product_code, item_no]\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def scraper_price_sale(self, data, product_code):\n",
    "        on_sale_sku = data.get(\"onSaleChildSkus\")\n",
    "        on_sale_sku_text = str(on_sale_sku)\n",
    "\n",
    "        listPrice_pattern = r\"\\'listPrice\\': \\'\\$[0-9]{0,5}.[0-9]{0,5}\"\n",
    "        salePrice_pattern = r\"\\'salePrice\\': \\'\\$[0-9]{0,5}.[0-9]{0,5}\"\n",
    "        skuId_pattern = r\"\\'skuId\\': \\'[0-9]{5,10}\"\n",
    "\n",
    "        listPrice_list = re.findall(listPrice_pattern, on_sale_sku_text)\n",
    "        salePrice_list = re.findall(salePrice_pattern, on_sale_sku_text)\n",
    "        skuId_list = re.findall(skuId_pattern, on_sale_sku_text)\n",
    "        \n",
    "        if len(skuId_list) == 0:\n",
    "            return None\n",
    "        elif len(listPrice_list) == len(salePrice_list) and len(listPrice_list) == len(skuId_list):\n",
    "            scraped_data = []\n",
    "            i = 0\n",
    "            for skuIds in skuId_list:\n",
    "                list_prices = listPrice_list[i]\n",
    "                sale_prices = salePrice_list[i]\n",
    "                \n",
    "                item_no = int(skuIds.split(\":\")[1].replace(\"'\", \"\").replace(\" \", \"\"))\n",
    "                list_price = float(list_prices.split(\":\")[1].replace(\"'\", \"\").replace(\" \", \"\").replace(\"$\",\"\"))\n",
    "                sale_price = float(sale_prices.split(\":\")[1].replace(\"'\", \"\").replace(\" \", \"\").replace(\"$\",\"\"))\n",
    "                is_sale = 1\n",
    "                i += 1\n",
    "                scraped_data.append([list_price, sale_price, is_sale, datetime.now(), product_code, item_no])\n",
    "            return scraped_data\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def insert_data(self, data, vertical):\n",
    "        # update product sales\n",
    "        query = f'''\n",
    "                update sephora_{vertical}_data_sale set list_price = %s, sale_price = %s, is_sale = %s, update_date = %s\n",
    "                where product_code = %s and item_no = %s;\n",
    "            '''\n",
    "        while True:\n",
    "            try:\n",
    "                self.ds_cur.execute(query, data)\n",
    "                self.ds_conn.commit()\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(100)\n",
    "                self.__close__()\n",
    "                self.__conn__()\n",
    "                print(\"DB 연결 끊김 ... 재연결 성공!\")\n",
    "    \n",
    "    def update_data(self, product_code, vertical):\n",
    "        product_code = product_code['product_code']\n",
    "        price_data = []\n",
    "        url = f'https://www.sephora.com/api/catalog/products/{product_code}?preferedSku=&includeConfigurableSku=true&passkey=caQ0pQXZTqFVYA1yYnnJ9emgUiW59DXA85Kxry8Ma02HE'\n",
    "        res_data = json_iterator(url)\n",
    "        status = 0\n",
    "        if res_data is None:\n",
    "            status -= 1\n",
    "        else:\n",
    "            scraped_data = self.scraper_price(res_data, product_code)\n",
    "            if scraped_data is None:\n",
    "                status -= 1\n",
    "            else:\n",
    "                self.insert_data(scraped_data, vertical)\n",
    "                price_data.append(scraped_data)\n",
    "                status += 1\n",
    "                \n",
    "            scraped_datas = self.scraper_price_sale(res_data, product_code)\n",
    "            if scraped_datas is None:\n",
    "                status -= 1\n",
    "            else:\n",
    "                for scraped_data in scraped_datas:\n",
    "                    self.insert_data(scraped_data, vertical)\n",
    "                    price_data.append(scraped_data)\n",
    "                status += 1\n",
    "        \n",
    "        return price_data, status\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    sale = UpdateProductSale()\n",
    "    verticals = ['face_base', 'eye', 'lip_color', 'moisturizers', 'cheek', 'treatments', 'masks', 'eye_care', 'body_care', 'mens', 'fragrance_men', 'fragrance_women', 'wellness', 'cleansers']\n",
    "    price_data_dict = {}\n",
    "    for vertical in tqdm(verticals):\n",
    "        sale.__conn__()\n",
    "        product_codes = sale.get_data(vertical)\n",
    "        sale.insert_data_new(vertical)\n",
    "\n",
    "        status_info, price_datas = [], []\n",
    "        for product_code in tqdm(product_codes):\n",
    "            price_data, status = sale.update_data(product_code, vertical)\n",
    "            \n",
    "            status_info.append([product_code, status])\n",
    "            price_datas += price_data\n",
    "        \n",
    "        price_data_dict[vertical] = price_datas\n",
    "        sale.__close__()\n",
    "        print(f'{vertical} product status update 완료!')\n",
    "        \n",
    "    return price_data_dict\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale = UpdateProductSale()\n",
    "verticals = ['face_base', 'eye', 'lip_color', 'moisturizers', 'cheek', 'treatments', 'masks', 'eye_care', 'body_care', 'mens', 'fragrance_men', 'fragrance_women', 'wellness', 'cleansers']\n",
    "price_data_dict = {}\n",
    "for vertical in tqdm(verticals):\n",
    "    sale.__conn__()\n",
    "    product_codes = sale.get_data(vertical)\n",
    "    sale.insert_data_new(vertical)\n",
    "\n",
    "    status_info, price_datas = [], []\n",
    "    for product_code in tqdm(product_codes):\n",
    "        price_data, status = sale.update_data(product_code, vertical)\n",
    "        \n",
    "        status_info.append([product_code, status])\n",
    "        price_datas += price_data\n",
    "    \n",
    "    price_data_dict[vertical] = price_datas\n",
    "    sale.__close__()\n",
    "    print(f'{vertical} product status update 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = db_glamai.get_tbl('sephora_cleansers_data_sale')\n",
    "# df_status_test = db_glamai.get_tbl('sephora_cleansers_data_status')\n",
    "\n",
    "# db_glamai.engine_upload(upload_df=df_test, table_name='sephora_test_data_sale', if_exists_option='replace')\n",
    "# db_glamai.engine_upload(upload_df=df_status_test, table_name='sephora_test_data_status', if_exists_option='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Product update: sephora_vertical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(raw_price):\n",
    "    if '(' in raw_price:\n",
    "        price = raw_price[:raw_price.find('(')]\n",
    "    else:\n",
    "        price = raw_price.split(' ')[0]\n",
    "    return price\n",
    "\n",
    "\n",
    "def get_sku_info(sku_data):\n",
    "        result = {}\n",
    "        main_url = 'https://www.sephora.com'\n",
    "        image_list = sku_data.get('alternateImages')\n",
    "        if image_list:\n",
    "            result['swatch'] = 'https://www.sephora.com' + image_list[0]['image450']\n",
    "        else:\n",
    "            result['swatch'] = None\n",
    "        color_value = sku_data.get('variationValue')\n",
    "        color_desc = sku_data.get('variationDesc')\n",
    "        color = []\n",
    "        if color_value:\n",
    "            color.append(color_value)\n",
    "        if color_desc:\n",
    "            color.append(color_desc)\n",
    "        color = ' - '.join(color)\n",
    "\n",
    "        result['color'] = color\n",
    "        price = sku_data['listPrice'][1:]\n",
    "        result['item_no'] = sku_data['skuId']\n",
    "        result['price'] = get_price(price)\n",
    "        result['size'] = sku_data.get('size')\n",
    "        result['item_no'] = sku_data.get('skuId', 0)\n",
    "        result['url'] = main_url + sku_data.get('targetUrl')\n",
    "        result['max_purchase_quantity'] = sku_data.get('maxPurchaseQuantity', 0)\n",
    "        result['bigvisual'] = main_url + sku_data['skuImages']['image450']\n",
    "        result['palette'] = sku_data.get('smallImage')\n",
    "        result['ingredients'] = sku_data.get('ingredientDesc')\n",
    "        result['free_shipping'] = sku_data.get('isFreeShippingSku', 0)\n",
    "        result['gift_wrappable'] = sku_data.get('isGiftWrappable', 0)\n",
    "        result['limited_edition'] = sku_data.get('isLimitedEdition', 0)\n",
    "        result['limited_quantity'] = sku_data.get('isLimitedQuantity', 0)\n",
    "        result['limited_time_offer'] = sku_data.get('isLimitedTimeOffer', 0)\n",
    "        result['natural_organic'] = sku_data.get('isNaturalOrganic', 0)\n",
    "        result['online_only'] = sku_data.get('isOnlineOnly', 0)\n",
    "        result['sephora_exclusive'] = sku_data.get('isSephoraExclusive', 0)\n",
    "        result['out_of_stock'] = sku_data.get('is_out_of_stock', 0)\n",
    "        result['max_purchase_quantity'] = sku_data.get('maxPurchaseQuantity', 0)\n",
    "        result['paypal_restrict'] = sku_data.get('isPaypalRestricted', 0)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_product_info(product_code): # 100개 정도\n",
    "        url = f'https://www.sephora.com/api/catalog/products/{product_code}?preferedSku=&includeConfigurableSku=true'\n",
    "        # response = requests.get(url, headers=self.get_headers())\n",
    "        # if 'errorCode' in response.text:\n",
    "        #     print(product_code, 'error')\n",
    "        #     time.sleep(random.randint(5,10))\n",
    "        #     response = requests.get(url, headers=self.get_headers())\n",
    "        #     if 'errorCode' in response.text:\n",
    "        #         print(product_code, 'repeated error')\n",
    "        #         return None\n",
    "        product_list = []\n",
    "        global ymal_sku\n",
    "        global result\n",
    "        global response_data\n",
    "        \n",
    "        response_data = json_iterator(url)\n",
    "        if response_data is None:\n",
    "            print(product_code, 'is None')\n",
    "            return None\n",
    "        elif 'errorCode' in response_data.text:\n",
    "            print(product_code, 'is None')\n",
    "            return None\n",
    "        else:\n",
    "            current_sku = response_data.get('currentSku')\n",
    "            if current_sku is None:\n",
    "                pass\n",
    "            else:\n",
    "                status = 1\n",
    "                try:\n",
    "                    result = get_sku_info(current_sku)\n",
    "                except Exception as e:\n",
    "                    status = 0\n",
    "                    print(e, product_code)\n",
    "        \n",
    "        if status == 1:\n",
    "            ymal_sku = response_data.get('ymalSkus')\n",
    "            if ymal_sku is None:\n",
    "                use_with = ''\n",
    "            else:\n",
    "                use_with = []\n",
    "                for ymal in ymal_sku:\n",
    "                    use_with.append(ymal['productId'])\n",
    "                use_with = ','.join(use_with)\n",
    "            result['use_with'] = use_with\n",
    "            \n",
    "            # brand\n",
    "            try:\n",
    "                result['brand'] = response_data.get('brand')['displayName']\n",
    "            except Exception as e:\n",
    "                result['brand'] = ''\n",
    "                print(e, product_code, '** brand is None **')\n",
    "\n",
    "            result['product_code'] = product_code\n",
    "            result['is_sale'] = response_data.get('onSaleSku', 0)\n",
    "            result['main_sku_check'] = 1\n",
    "            result['like_count'] = response_data.get('lovesCount', 0)\n",
    "            result['review_count'] = response_data.get('reviews', 0)\n",
    "            result['what_it_is'] = response_data.get('quickLookDescription')\n",
    "            rating = response_data.get('rating', 0)\n",
    "            result['rating'] = round(rating * 2) / 2\n",
    "            result['product_name'] = response_data.get('displayName')\n",
    "            \n",
    "            # details 부분 추가\n",
    "            result['details'] = response_data.get('longDescription')\n",
    "            if response_data.get('longDescription') is None:\n",
    "                result['details'] = \"\"\n",
    "            else:\n",
    "                result['details'] = html.unescape(result['details']).replace(\"<b>\", \"\").replace(\"</b>\", \"\").replace(\"<br>\", \"\").replace(\"</br>\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "                result['details'] = re.sub('<.+?>', '', result['details'])\n",
    "                \n",
    "            product_list.append(result)\n",
    "        \n",
    "        # regularChildSkus\n",
    "        sku_list = response_data.get('regularChildSkus')\n",
    "        if sku_list is None:\n",
    "            sku_list = response_data.get('onSaleChildSkus', [])\n",
    "\n",
    "        for sku in sku_list:\n",
    "            status = 1\n",
    "            try:\n",
    "                result = get_sku_info(sku)\n",
    "            except Exception as e:\n",
    "                status = 0\n",
    "                print(e, product_code)\n",
    "            if status == 1:\n",
    "                result['product_code'] = product_code\n",
    "                # brand\n",
    "                try:\n",
    "                    result['brand'] = response_data.get('brand')['displayName']\n",
    "                except Exception as e:\n",
    "                    result['brand'] = ''\n",
    "                    print(e, product_code, '** brand is None **')\n",
    "                result['is_sale'] = response_data.get('onSaleSku', 0)\n",
    "                result['main_sku_check'] = 0\n",
    "                try:\n",
    "                    result['use_with'] = use_with\n",
    "                except:\n",
    "                    pass\n",
    "                result['like_count'] = response_data.get('lovesCount', 0)\n",
    "                result['review_count'] = response_data.get('reviews', 0)\n",
    "                result['what_it_is'] = response_data.get('quickLookDescription')\n",
    "                rating = response_data.get('rating', 0)\n",
    "                result['rating'] = round(rating * 2) / 2\n",
    "                result['product_name'] = response_data.get('displayName')\n",
    "                \n",
    "                # details 부분 추가\n",
    "                result['details'] = response_data.get('longDescription')\n",
    "                if response_data.get('longDescription') is None:\n",
    "                    result['details'] = \"\"\n",
    "                else:\n",
    "                    result['details'] = html.unescape(result['details']).replace(\"<b>\", \"\").replace(\"</b>\", \"\").replace(\"<br>\", \"\").replace(\"</br>\", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "                    result['details'] = re.sub('<.+?>', '', result['details'])\n",
    "                product_list.append(result)\n",
    "\n",
    "        return product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sephora_product.vertical_data import VerticalData\n",
    "vd = VerticalData()\n",
    "vertical = 'eye'\n",
    "remain_product = vd.get_vertical_product(vertical)\n",
    "products = vd.get_product_list(vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = {}\n",
    "error_, _error, product_infos = [], [], []\n",
    "for product in tqdm(products[:200]):\n",
    "    product_code = product['product_code']\n",
    "    product_info = get_product_info(product_code)\n",
    "    product_infos += product_info\n",
    "    \n",
    "    \n",
    "    # url = f'https://www.sephora.com/api/catalog/products/{product_code}?preferedSku=&includeConfigurableSku=true'\n",
    "    # response_data = json_iterator(url)\n",
    "    \n",
    "    # status = 1\n",
    "    # try:        \n",
    "    #     ymalSkus_ = response_data.get('ymalSkus')\n",
    "    # except KeyError:\n",
    "    #     status -= 1\n",
    "    #     error_.append(url)\n",
    "    # try:\n",
    "    #     _ymalSkus = response_data['ymalSkus']\n",
    "    # except KeyError:\n",
    "    #     status -= 1\n",
    "    #     _error.append(url)\n",
    "    # if status == 1:\n",
    "    #     product_infos.append([ymalSkus_, _ymalSkus])\n",
    "    # error[product_code] = status\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_df = pd.DataFrame(product_infos)\n",
    "info_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Sephora product status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sephora_update.status import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verticals = ['face_base', 'eye', 'lip_color', 'moisturizers', 'cheek', 'treatments', 'masks', 'eye_care', 'body_care', 'mens', 'fragrance_men', 'fragrance_women', 'wellness', 'cleansers']\n",
    "# for vertical in tqdm(verticals):\n",
    "#     table_name = f'{vertical}_product_info'\n",
    "#     info_df = db_glamai.get_tbl(table_name, ['product_code', 'item_no', 'url', 'price', 'regist_date'])\n",
    "#     info_df_dedup = info_df.drop_duplicates(subset=['product_code', 'item_no'], keep='first')\n",
    "#     info_status = []\n",
    "#     for info in tqdm(info_df_dedup.values):\n",
    "#         product_code = info[0]\n",
    "#         item_no = info[1]\n",
    "#         url = info[2]\n",
    "#         price_org = info[3]\n",
    "#         regist_date = info[4]\n",
    "#         price, is_use = get_status(url, item_no)\n",
    "#         update_date = datetime.today()\n",
    "#         if price is None:\n",
    "#             price = price_org\n",
    "#         info_status.append([product_code, item_no, url, price, is_use, regist_date, update_date])\n",
    "        \n",
    "#     upload_table = f'sephora_{vertical}_data_status'\n",
    "#     columns = ['product_code', 'item_no', 'url', 'price', 'is_use', 'regist_date', 'update_date']\n",
    "#     upload_df = pd.DataFrame(info_status, columns=columns)\n",
    "    \n",
    "#     db_glamai.create_table(upload_df=upload_df, table_name=upload_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "\n",
    "# vertical = 'test'\n",
    "# table_name = f'{vertical}_product_info'\n",
    "# info_df = db_glamai.get_tbl(table_name, ['product_code', 'item_no', 'url', 'price', 'regist_date'])\n",
    "# info_df_dedup = info_df.drop_duplicates(subset=['product_code', 'item_no'], keep='first')\n",
    "# info_status = []\n",
    "# for info in tqdm(info_df_dedup.values):\n",
    "#     product_code = info[0]\n",
    "#     item_no = info[1]\n",
    "#     url = info[2]\n",
    "#     price_org = info[3]\n",
    "#     regist_date = info[4]\n",
    "#     price, is_use = get_status(url, item_no)\n",
    "#     update_date = datetime.today()\n",
    "#     if price is None:\n",
    "#         price = price_org\n",
    "#     info_status.append([product_code, item_no, url, price, is_use, regist_date, update_date])\n",
    "    \n",
    "# upload_table = f'sephora_{vertical}_data_status'\n",
    "# columns = ['product_code', 'item_no', 'url', 'price', 'is_use', 'regist_date', 'update_date']\n",
    "# upload_df = pd.DataFrame(info_status, columns=columns)\n",
    "# db_glamai.create_table(upload_df, upload_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "\n",
    "# test_df = db_glamai.get_tbl('face_base_product_info')\n",
    "# test_product_info = test_df.sample(50)\n",
    "# db_glamai.engine_upload(test_product_info, 'test_product_info', if_exists_option=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Search keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sephora_keyword.search_keyword import update_search_keywords, db_distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sephora_keyword.search_keyword import update_search_keywords, db_distinction\n",
    "total_df = update_search_keywords()\n",
    "db_distinction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keyowrds_df = db_glamai.get_tbl('glamai_search_keywords')\n",
    "search_keyowrds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keyowrds_df = db_glamai.get_tbl('glamai_search_keywords')\n",
    "search_keyowrds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keyowrds_df = db_glamai.get_tbl('glamai_search_keywords')\n",
    "search_keyowrds_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Affiliate Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_amazon(url):\n",
    "\n",
    "#     # wd = get_url(url)\n",
    "#     wd = get_url(url)\n",
    "#     soup = BeautifulSoup(wd.page_source, 'lxml')\n",
    "#     if soup is None:\n",
    "#         print(\"soup is None\")\n",
    "#         wd.quit()\n",
    "#         return None\n",
    "#     else:    \n",
    "#         # Check page status\n",
    "#         if soup.find('div', {'id': 'g'}) is None:\n",
    "#             page_status = 1\n",
    "#         else:\n",
    "#             page_status = 0\n",
    "#             avaliability_txt, price = None, None\n",
    "#             price_normal, price_sale, is_sale, is_use = 0, 0, 0, 0\n",
    "        \n",
    "#         if page_status == 1:\n",
    "#             # Check currently unavailable\n",
    "#             if soup.find('div', {'id': 'availability'}) is None:\n",
    "#                 avaliability_txt = None\n",
    "#             else:\n",
    "#                 avaliability = soup.find('div', {'id': 'availability'})\n",
    "#                 avaliability_txt = avaliability.find('span').text.strip()\n",
    "\n",
    "#             # Check price\n",
    "#             if soup.find('div', 'a-section a-spacing-none aok-align-center') is not None:\n",
    "#                 price_area = soup.find('div', 'a-section a-spacing-none aok-align-center')\n",
    "#                 if price_area.find('span', 'a-offscreen') is None:\n",
    "#                     price = None\n",
    "#                     price_normal, price_sale, is_sale, is_use = 0, 0, 0, 0\n",
    "#                 else:\n",
    "#                     is_use = 1\n",
    "#                     price = price_area.find('span', 'a-offscreen').text\n",
    "#                     price_sale = round(float(price[1:]), 2)\n",
    "#                     if soup.find('span', 'a-size-small a-color-secondary aok-align-center basisPrice') is None:\n",
    "#                         price_normal = price_sale\n",
    "#                         price_sale = 0\n",
    "#                         is_sale = 0\n",
    "#                     else:\n",
    "#                         price_area = soup.find('span', 'a-size-small a-color-secondary aok-align-center basisPrice')\n",
    "#                         if price_area.find('span', 'a-offscreen') is None:\n",
    "#                             price_normal = price_sale\n",
    "#                             price_sale = 0\n",
    "#                             is_sale = 0\n",
    "#                         else:\n",
    "#                             price = price_area.find('span', 'a-offscreen').text\n",
    "#                             price_normal = round(float(price[1:]), 2)   \n",
    "#                             if price_normal > price_sale:\n",
    "#                                 is_sale = 1\n",
    "#                             else:\n",
    "#                                 is_sale = -1\n",
    "                                \n",
    "#             elif soup.find('div', {'id': 'corePrice_desktop'}) is not None:\n",
    "#                 price = None\n",
    "#                 price_area = soup.find('div', {'id': 'corePrice_desktop'})\n",
    "                \n",
    "#                 is_sale, is_use = 0, 0\n",
    "#                 try:\n",
    "#                     price = price_area.find('span', 'a-price a-text-price a-size-base').find('span', 'a-offscreen').text\n",
    "#                     price_normal = round(float(price[1:]), 2)\n",
    "#                     is_use = 1\n",
    "#                 except:\n",
    "#                     price_normal = 0\n",
    "                    \n",
    "#                 try:\n",
    "#                     price = price_area.find('span', 'a-price a-text-price a-size-medium apexPriceToPay').find_all('span', 'a-offscreen')[0].text\n",
    "#                     price_sale = round(float(price[1:]), 2)\n",
    "#                 except:\n",
    "#                     price_sale = 0\n",
    "                    \n",
    "#                 if price_normal > price_sale:\n",
    "#                     is_sale = 1\n",
    "#                 else:\n",
    "#                     price_normal = price_sale\n",
    "#                     price_sale = 0\n",
    "    \n",
    "#             else:\n",
    "#                 price_normal, price_sale, is_sale, is_use = 0, 0, 0, 0\n",
    "                \n",
    "\n",
    "#         wd.quit()\n",
    "#         return [url, page_status, avaliability_txt, price_normal, price_sale, is_sale, is_use, price]\n",
    "\n",
    "# def get_data():\n",
    "#     df_price = db_glamai.get_tbl('affiliate_price')\n",
    "#     df_amazon = df_price[df_price.affiliate_type=='amazon']\n",
    "    \n",
    "#     return df_amazon\n",
    "    \n",
    "# def _crawling(value):\n",
    "#     product_code = value[0]\n",
    "#     item_no = value[1]\n",
    "#     affiliate_type = 'amazon'\n",
    "#     affiliate_url = value[3]\n",
    "#     affiliate_image = value[4]\n",
    "#     regist_date = value[9]\n",
    "    \n",
    "#     data = get_data_amazon(affiliate_url)\n",
    "#     if data is None:\n",
    "#         pass\n",
    "#     else:\n",
    "#         data = [product_code, item_no, affiliate_type, affiliate_image, regist_date] + data\n",
    "        \n",
    "#     return data\n",
    "\n",
    "# def upload_data(data):\n",
    "#     columns = ['product_code', 'item_no', 'affiliate_type', 'affiliate_image', 'regist_date', 'affiliate_url', 'page_status', 'avaliability_txt', 'price', 'sale_price', 'is_sale', 'is_use', 'price_']\n",
    "#     crawling_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "#     upload_columns = ['product_code', 'item_no', 'affiliate_type', 'affiliate_url', 'affiliate_image',  'price', 'sale_price', 'is_sale', 'is_use', 'regist_date']\n",
    "#     upload_df = crawling_df.loc[:, upload_columns]\n",
    "#     upload_df.loc[:, 'update_date'] = datetime.today()\n",
    "#     # db_jangho.create_table(upload_df=upload_df, table_name='affiliate_price_update_amazon')\n",
    "    \n",
    "#     return crawling_df, upload_df\n",
    "\n",
    "# def main():\n",
    "#     df_amazon = get_data()\n",
    "#     datas, error = [], []\n",
    "#     for value in tqdm(df_amazon.values):\n",
    "#         data = _crawling(value)\n",
    "#         if data is None:\n",
    "#             affiliate_url = value[3]\n",
    "#             error.append(affiliate_url)\n",
    "#         else:\n",
    "#             datas.append(data)\n",
    "#     crawling_df, upload_df = upload_data(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_amazon = get_data()\n",
    "# datas, error = [], []\n",
    "# for value in tqdm(df_amazon.values):\n",
    "#     data = _crawling(value)\n",
    "#     if data is None:\n",
    "#         affiliate_url = value[3]\n",
    "#         error.append(affiliate_url)\n",
    "#     else:\n",
    "#         datas.append(data)\n",
    "# crawling_df, upload_df = upload_data(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('avaliability_txt').count()\n",
    "# urls = df[(df.avaliability_txt != 'Currently unavailable.') & (df.price_.isnull())].affiliate_url.unique().tolist()\n",
    "\n",
    "# datas = []\n",
    "# for url in tqdm(urls):\n",
    "#     wd = get_url(url)\n",
    "#     soup = BeautifulSoup(wd.page_source, 'lxml')\n",
    "#     if soup is None:\n",
    "#         print(\"soup is None\", url)\n",
    "#         wd.quit()\n",
    "#     else:    \n",
    "#         price = None\n",
    "        \n",
    "#         if soup.find('div', {'id': 'corePrice_desktop'}) is None:\n",
    "#             price_normal = 0\n",
    "#         else:\n",
    "#             price_area = soup.find('div', {'id': 'corePrice_desktop'})\n",
    "#             try:\n",
    "#                 price = price_area.find('span', 'a-price a-text-price a-size-base').find('span', 'a-offscreen').text\n",
    "#                 price_normal = round(float(price[1:]), 2)\n",
    "#             except:\n",
    "#                 price_normal = 0\n",
    "            \n",
    "#             try:\n",
    "#                 price = price_area.find('span', 'a-price a-text-price a-size-medium apexPriceToPay').find_all('span', 'a-offscreen')[0].text\n",
    "#                 price_sale = round(float(price[1:]), 2)\n",
    "#             except:\n",
    "#                 price_sale = 0\n",
    "                \n",
    "#         datas.append([url, price_normal, price_sale, price]) \n",
    "\n",
    "# _df = pd.DataFrame(datas, columns=['url', 'price', 'sale', 'price_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "`affiliate_price` Import Time: 1.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 8.15M/8.15M [00:00<00:00, 12.5MB/s]\n",
      " 61%|██████▏   | 2809/4584 [2:22:29<1:46:36,  3.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Error: Message: unknown error: net::ERR_INTERNET_DISCONNECTED\n",
      "  (Session info: headless chrome=105.0.5195.125)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001045ef788 chromedriver + 4515720\n",
      "1   chromedriver                        0x00000001045739d3 chromedriver + 4008403\n",
      "2   chromedriver                        0x000000010420612a chromedriver + 413994\n",
      "3   chromedriver                        0x00000001041ffd8e chromedriver + 388494\n",
      "4   chromedriver                        0x00000001041f34ad chromedriver + 337069\n",
      "5   chromedriver                        0x00000001041f4370 chromedriver + 340848\n",
      "6   chromedriver                        0x00000001041f3718 chromedriver + 337688\n",
      "7   chromedriver                        0x00000001041f2bd4 chromedriver + 334804\n",
      "8   chromedriver                        0x00000001041f1a87 chromedriver + 330375\n",
      "9   chromedriver                        0x00000001041f1e02 chromedriver + 331266\n",
      "10  chromedriver                        0x0000000104207d2e chromedriver + 421166\n",
      "11  chromedriver                        0x000000010426e40f chromedriver + 840719\n",
      "12  chromedriver                        0x000000010425a7d2 chromedriver + 759762\n",
      "13  chromedriver                        0x000000010426dbd9 chromedriver + 838617\n",
      "14  chromedriver                        0x000000010425a603 chromedriver + 759299\n",
      "15  chromedriver                        0x0000000104230990 chromedriver + 588176\n",
      "16  chromedriver                        0x0000000104231a75 chromedriver + 592501\n",
      "17  chromedriver                        0x00000001045bf6cd chromedriver + 4318925\n",
      "18  chromedriver                        0x00000001045c3f35 chromedriver + 4337461\n",
      "19  chromedriver                        0x00000001045cb1ff chromedriver + 4366847\n",
      "20  chromedriver                        0x00000001045c4c5a chromedriver + 4340826\n",
      "21  chromedriver                        0x000000010459ac2c chromedriver + 4168748\n",
      "22  chromedriver                        0x00000001045e14f8 chromedriver + 4457720\n",
      "23  chromedriver                        0x00000001045e1693 chromedriver + 4458131\n",
      "24  chromedriver                        0x00000001045f6a3e chromedriver + 4545086\n",
      "25  libsystem_pthread.dylib             0x00007ff813fe64e1 _pthread_start + 125\n",
      "26  libsystem_pthread.dylib             0x00007ff813fe1f6b thread_start + 15\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error: Message: unknown error: net::ERR_INTERNET_DISCONNECTED\n",
      "  (Session info: headless chrome=105.0.5195.125)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000109366788 chromedriver + 4515720\n",
      "1   chromedriver                        0x00000001092ea9d3 chromedriver + 4008403\n",
      "2   chromedriver                        0x0000000108f7d12a chromedriver + 413994\n",
      "3   chromedriver                        0x0000000108f76d8e chromedriver + 388494\n",
      "4   chromedriver                        0x0000000108f6a4ad chromedriver + 337069\n",
      "5   chromedriver                        0x0000000108f6b370 chromedriver + 340848\n",
      "6   chromedriver                        0x0000000108f6a718 chromedriver + 337688\n",
      "7   chromedriver                        0x0000000108f69bd4 chromedriver + 334804\n",
      "8   chromedriver                        0x0000000108f68a87 chromedriver + 330375\n",
      "9   chromedriver                        0x0000000108f68e02 chromedriver + 331266\n",
      "10  chromedriver                        0x0000000108f7ed2e chromedriver + 421166\n",
      "11  chromedriver                        0x0000000108fe540f chromedriver + 840719\n",
      "12  chromedriver                        0x0000000108fd17d2 chromedriver + 759762\n",
      "13  chromedriver                        0x0000000108fe4bd9 chromedriver + 838617\n",
      "14  chromedriver                        0x0000000108fd1603 chromedriver + 759299\n",
      "15  chromedriver                        0x0000000108fa7990 chromedriver + 588176\n",
      "16  chromedriver                        0x0000000108fa8a75 chromedriver + 592501\n",
      "17  chromedriver                        0x00000001093366cd chromedriver + 4318925\n",
      "18  chromedriver                        0x000000010933af35 chromedriver + 4337461\n",
      "19  chromedriver                        0x00000001093421ff chromedriver + 4366847\n",
      "20  chromedriver                        0x000000010933bc5a chromedriver + 4340826\n",
      "21  chromedriver                        0x0000000109311c2c chromedriver + 4168748\n",
      "22  chromedriver                        0x00000001093584f8 chromedriver + 4457720\n",
      "23  chromedriver                        0x0000000109358693 chromedriver + 4458131\n",
      "24  chromedriver                        0x000000010936da3e chromedriver + 4545086\n",
      "25  libsystem_pthread.dylib             0x00007ff813fe64e1 _pthread_start + 125\n",
      "26  libsystem_pthread.dylib             0x00007ff813fe1f6b thread_start + 15\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4584/4584 [3:38:41<00:00,  2.86s/it]    \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (13, 1), indices imply (13, 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yeonseosla/Desktop/glamai/_test.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yeonseosla/Desktop/glamai/_test.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yeonseosla/Desktop/glamai/_test.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         datas\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yeonseosla/Desktop/glamai/_test.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m crawling_df, upload_df \u001b[39m=\u001b[39m _upload(data)\n",
      "File \u001b[0;32m~/Desktop/glamai/src/affiliate/amazon.py:132\u001b[0m, in \u001b[0;36m_upload\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_upload\u001b[39m(data):\n\u001b[1;32m    131\u001b[0m     columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mproduct_code\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mitem_no\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maffiliate_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maffiliate_image\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mregist_date\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maffiliate_url\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpage_status\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mavaliability_txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msale_price\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis_sale\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis_use\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprice_\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m     crawling_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data, columns\u001b[39m=\u001b[39;49mcolumns)\n\u001b[1;32m    134\u001b[0m     upload_columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mproduct_code\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mitem_no\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maffiliate_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maffiliate_url\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maffiliate_image\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msale_price\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis_sale\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis_use\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mregist_date\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    135\u001b[0m     upload_df \u001b[39m=\u001b[39m crawling_df\u001b[39m.\u001b[39mloc[:, upload_columns]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.10/site-packages/pandas/core/frame.py:737\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    729\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    730\u001b[0m             arrays,\n\u001b[1;32m    731\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    734\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 737\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    738\u001b[0m             data,\n\u001b[1;32m    739\u001b[0m             index,\n\u001b[1;32m    740\u001b[0m             columns,\n\u001b[1;32m    741\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    742\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    743\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    744\u001b[0m         )\n\u001b[1;32m    745\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    747\u001b[0m         {},\n\u001b[1;32m    748\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    751\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    752\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    347\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[1;32m    348\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[1;32m    349\u001b[0m )\n\u001b[0;32m--> 351\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    420\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    421\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[0;32m--> 422\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (13, 1), indices imply (13, 13)"
     ]
    }
   ],
   "source": [
    "from affiliate.amazon import get_data, _crawling, _upload\n",
    "df_amazon = get_data()\n",
    "datas, error = [], []\n",
    "for value in tqdm(df_amazon.values):\n",
    "    data = _crawling(value)\n",
    "    if data is None:\n",
    "        affiliate_url = value[3]\n",
    "        error.append(affiliate_url)\n",
    "    else:\n",
    "        datas.append(data)\n",
    "crawling_df, upload_df = _upload(datas)\n",
    "# db_jangho.create_table(upload_df=upload_df, table_name='affiliate_price_update_amazon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(action=\"ignore\")\n",
    "# pd.set_option('display.max_columns', 100)\n",
    "# pd.set_option('display.max_rows', 3000)\n",
    "# pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def get_data_ulta(url):\n",
    "    driver = get_url(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    big_price_txt, small_price_txt = None, None\n",
    "    if driver is None:\n",
    "        normal_price, sale_price = 0, 0\n",
    "        is_sale, is_use = 0, 0\n",
    "        status = -2\n",
    "    \n",
    "    else:\n",
    "        # small price가 있으면 -> small price가 정상가, big price가 할인가\n",
    "        # small price가 없으면 -> big price가 정상가\n",
    "        is_sale, is_use = 1, 1\n",
    "        status = 1\n",
    "        big_price_flag = False           \n",
    "        try:\n",
    "            big_price_txt = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div/div/main/div/div/div[3]/div/div[3]/span[1]').text\n",
    "            big_price = ''.join(x for x in big_price_txt if x not in \"Price\\n$\").replace('Sal', '').strip()\n",
    "            big_price_flag = True\n",
    "\n",
    "            small_price_txt = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div/div/main/div/div/div[3]/div/div[3]/span[3]').text\n",
    "            small_price = ''.join(x for x in small_price_txt if x not in \"Original Price\\n$\")\n",
    "            reg_drop = r'[\\(\\)vu]+'\n",
    "            small_price = re.sub(reg_drop, '', small_price)\n",
    "            \n",
    "            normal_price = small_price\n",
    "            sale_price = big_price\n",
    "\n",
    "            # check sale\n",
    "            normal_price = round(float(normal_price), 2)\n",
    "            sale_price = round(float(sale_price), 2)\n",
    "            if normal_price > sale_price:\n",
    "                pass\n",
    "            else:\n",
    "                sale_price = 0\n",
    "                is_sale = 0\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            if big_price_flag:\n",
    "                normal_price = big_price\n",
    "                sale_price = 0\n",
    "                is_sale = 0\n",
    "            else:\n",
    "                normal_price, sale_price = 0, 0\n",
    "                is_sale, is_use = 0, 0\n",
    "                status = 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            normal_price, sale_price = 0, 0\n",
    "            is_sale, is_use = 0, 0\n",
    "            status = -1\n",
    "            \n",
    "    normal_price = round(float(normal_price), 2)\n",
    "    sale_price = round(float(sale_price), 2)\n",
    "    driver.quit()\n",
    "    return status, normal_price, sale_price, is_sale, is_use, big_price_txt, small_price_txt\n",
    "\n",
    "def get_data():\n",
    "    df_price = db_glamai.get_tbl('affiliate_price')\n",
    "    df_ulta = df_price[df_price.affiliate_type=='ulta']\n",
    "    \n",
    "    return df_ulta\n",
    "\n",
    "def _crawling(value):\n",
    "    product_code = value[0]\n",
    "    item_no = value[1]\n",
    "    affiliate_type = 'ulta'\n",
    "    affiliate_url = value[3]\n",
    "    affiliate_image = value[4]\n",
    "    regist_date = value[9]\n",
    "    \n",
    "    status, price, sale_price, is_sale, is_use, big_price_txt, small_price_txt = get_data_ulta(affiliate_url)\n",
    "    data = [product_code, item_no, affiliate_type, affiliate_url, affiliate_image, price, sale_price, is_sale, is_use, regist_date, status, big_price_txt, small_price_txt]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def _upload(data):\n",
    "    columns = ['product_code', 'item_no', 'affiliate_type', 'affiliate_url', 'affiliate_image', 'price', 'sale_price', 'is_sale', 'is_use', 'regist_date', 'status', 'big_price', 'small_price']\n",
    "    crawling_df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    upload_columns = ['product_code', 'item_no', 'affiliate_type', 'affiliate_url', 'affiliate_image',  'price', 'sale_price', 'is_sale', 'is_use', 'regist_date']\n",
    "    upload_df = crawling_df.loc[:, upload_columns]\n",
    "    upload_df.loc[:, 'update_date'] = datetime.today()\n",
    "    db_jangho.create_table(upload_df=upload_df, table_name='affiliate_price_update_ulta')\n",
    "    \n",
    "    return crawling_df, upload_df\n",
    "\n",
    "def main():\n",
    "    df_ulta = get_data()\n",
    "    datas, error = [], []\n",
    "    for value in tqdm(df_ulta.values):\n",
    "        data = _crawling(value)\n",
    "        if data is None:\n",
    "            affiliate_url = value[3]\n",
    "            error.append(affiliate_url)\n",
    "        else:\n",
    "            datas.append(data)\n",
    "    crawling_df, upload_df = _upload(datas)\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ulta = get_data()\n",
    "datas, error = [], []\n",
    "for value in tqdm(df_ulta.values):\n",
    "    data = _crawling(value)\n",
    "    if data is None:\n",
    "        affiliate_url = value[3]\n",
    "        error.append(affiliate_url)\n",
    "    else:\n",
    "        datas.append(data)\n",
    "crawling_df, upload_df = _upload(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulta_df = crawling_df.copy()\n",
    "\n",
    "ulta_df.groupby('is_use').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulta_df.groupby('is_sale').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulta_df.groupby('status').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulta_df.groupby('price').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulta_df.groupby('sale_price').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulta_df.groupby('big_price').count().index.tolist()\n",
    "ulta_df.groupby('small_price').count().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_columns = ['product_code', 'item_no', 'affiliate_type', 'affiliate_url', 'affiliate_image',  'price', 'sale_price', 'is_sale', 'is_use', 'regist_date']\n",
    "upload_df = ulta_df.loc[:, upload_columns]\n",
    "upload_df.loc[:, 'update_date'] = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_df = upload_df.sort_values(by=['product_code', 'item_no', 'regist_date', 'update_date'], ignore_index=True)\n",
    "db_jangho.create_table(upload_df=upload_df, table_name='affiliate_price_update_ulta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### update sale test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sephora_update.sales import update_sephora_sale\n",
    "vertical = 'test'\n",
    "data = update_sephora_sale(vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test table ## \n",
    "\n",
    "# sale_test = db_glamai.get_tbl('sephora_cleansers_data_sale')\n",
    "# status_test = db_glamai.get_tbl('sephora_cleansers_data_status')\n",
    "\n",
    "# db_glamai.engine_upload(upload_df=sale_test, table_name='sephora_test_data_sale', if_exists_option='replace')\n",
    "# db_glamai.engine_upload(upload_df=status_test, table_name='sephora_test_data_status', if_exists_option='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf4960e0944d0187d62a18875a8b32193841666fde4904048c0d8eeed8d33aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
